# ğŸ”¬ LLMscope

## Statistical Process Control for LLM Performance Monitoring

[![GitHub stars](https://img.shields.io/github/stars/Blb3D/LLMscope?style=social)](https://github.com/Blb3D/LLMscope/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/Blb3D/LLMscope?style=social)](https://github.com/Blb3D/LLMscope/network)
[![License](https://img.shields.io/badge/license-BSL--1.1-blue)](./LICENSE-BSL.txt)
[![Docker Ready](https://img.shields.io/badge/docker-ready-brightgreen)](https://hub.docker.com/r/blb3d/llmscope)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)

[ğŸš€ Live Demo](https://demo.llmscope.io) | [ğŸ“– Documentation](./docs) | [ï¿½ Contact](./CONTACT.md) | [ğŸ› Report Bug](https://github.com/Blb3D/LLMscope/issues)

## ğŸ¯ Quick Access

After running `docker-compose up -d`:

- **ğŸŒ Dashboard**: [http://localhost:8081](http://localhost:8081) - Main SPC monitoring interface
- **ğŸ”Œ API**: [http://localhost:8000](http://localhost:8000) - Backend API endpoints  
- **ğŸ“Š API Docs**: [http://localhost:8000/docs](http://localhost:8000/docs) - Interactive API documentation
- **ğŸ”‘ API Key**: Set via `LLMSCOPE_API_KEY` environment variable

---

<div align="center">
  <img src="https://github.com/Blb3D/LLMscope/assets/Blb3D/llmscope-dashboard-preview.gif" width="800" alt="LLMscope Dashboard showing real-time violation detection">
  <br>
  <i>Detecting performance degradation 10 minutes before failure using Nelson Rules</i>
</div>

### ğŸ¬ AI Copilot Demo Video

**Smart Fallback System in Action** (3 minutes - full workflow)

[![AI Copilot Demo](https://img.shields.io/badge/ğŸ¥_Watch_Demo-AI_Copilot_Smart_Fallback-blue?style=for-the-badge)](docs/assets/demos/ai-copilot-demo-smart-fallback.mp4)

See the complete AI workflow: violation detection â†’ intelligent analysis â†’ direct resolution. Features smart model detection, professional explanations, and seamless fallback from llama3.2:3b to 1b.

---

## ğŸš¨ The Problem

**Your LLM is failing silently.** Traditional monitoring only tells you AFTER it crashes. By then, users are angry and damage is done.

- âŒ **Random latency spikes** with no warning
- âŒ **Gradual performance degradation** goes unnoticed  
- âŒ **Alert fatigue** from dumb threshold-based monitoring
- âŒ **No statistical rigor** in existing LLM tools

## âœ¨ The Solution: Manufacturing-Grade Quality Control for AI

LLMscope brings **50 years of proven Statistical Process Control (SPC)** from manufacturing to LLM monitoring. The same math that keeps airplane parts from failing now protects your AI infrastructure.

### ğŸ¯ Key Features

- **ğŸ§  AI Copilot** - Intelligent violation analysis powered by Ollama:
  - **Smart Model Fallback** - Works with any available model (llama3.2:1b, 3b, etc.)
  - **3 Explanation Types** - Technical, Business, and Remediation perspectives
  - **Zero Setup** - Automatically detects and uses your Ollama models
  - **Professional Analysis** - Root cause analysis and actionable remediation steps

- **ğŸ“Š Nelson Rules Detection** - 8 statistical patterns that catch issues early:
  - R1: Points beyond 3Ïƒ (immediate issues)
  - R2: 9+ points on same side (process shift)
  - R3: 6+ trending points (degradation)
  - ...and 5 more advanced patterns

- **âš¡ Real-Time Monitoring** - Sub-second latency tracking with beautiful visualizations

- **ğŸ”” Smart Alerts** - Email/Slack notifications with context, not just "threshold exceeded"

- **ğŸ  Self-Hosted** - Your data never leaves your infrastructure (SOC2/HIPAA friendly)

- **ğŸ”Œ Universal Support** - Works with Ollama, OpenAI, Anthropic, Cohere, and any LLM API

## ğŸš€ Quick Start (2 minutes)

### Option 1: One-Line Install (Recommended)

```bash
curl -sSL https://raw.githubusercontent.com/Blb3D/LLMscope/main/install.sh | bash
```

### Option 2: Docker Compose

```bash
# Clone the repository
git clone https://github.com/Blb3D/LLMscope.git
cd LLMscope

# Start the stack
docker-compose up -d

# Open your browser
open http://localhost:8081
```

### ğŸ§  AI Copilot Setup (Optional but Recommended)

For intelligent violation analysis, install Ollama:

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Download a model (pick one):
ollama pull llama3.2:3b    # Best quality (2GB)
ollama pull llama3.2:1b    # Faster, smaller (1.3GB)

# LLMscope will auto-detect and use any available model!
```

**Note**: AI Copilot works with smart fallback - if you have `llama3.2:1b`, it uses that. If you upgrade to `llama3.2:3b`, it automatically switches. Zero configuration required!

### Option 3: Manual Setup

```bash
# Backend API
cd backend && pip install -r requirements.txt
python app.py

# Frontend (separate terminal)
cd frontend && npm install
npm run dev

# Monitor service (separate terminal)
cd monitor && python monitor_apis.py
```

## ğŸ“Š What Makes LLMscope Different?

<table>
<tr>
<th>Feature</th>
<th>LLMscope</th>
<th>Langfuse</th>
<th>DataDog</th>
<th>Helicone</th>
</tr>
<tr>
<td><b>AI Copilot Analysis</b></td>
<td>âœ… Ollama-powered</td>
<td>âŒ</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><b>Statistical Process Control</b></td>
<td>âœ… Full SPC</td>
<td>âŒ</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><b>Nelson Rules (8 patterns)</b></td>
<td>âœ…</td>
<td>âŒ</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><b>Predictive Detection</b></td>
<td>âœ… 10min early</td>
<td>âŒ</td>
<td>Limited</td>
<td>âŒ</td>
</tr>
<tr>
<td><b>Smart Model Fallback</b></td>
<td>âœ… Auto-detects</td>
<td>âŒ</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><b>Self-Hosted Option</b></td>
<td>âœ…</td>
<td>âœ…</td>
<td>âŒ</td>
<td>âŒ</td>
</tr>
<tr>
<td><b>Pricing</b></td>
<td>Free (self-hosted)</td>
<td>$59/mo (100k events)</td>
<td>$$$$ usage-based</td>
<td>$39/mo (100k logs)</td>
</tr>
</table>

## ğŸ¬ See It In Action

### Real Violation Detection

<!-- Future: Add violation detection GIF -->
<img src="https://via.placeholder.com/600x300/1e293b/06b6d4?text=Violation+Detection+Demo" width="600" alt="Violation detection in action">

In this real test, LLMscope detected a "cognitive load spike" pattern 10 minutes before Ollama would have crashed, allowing automatic intervention.

### Dashboard Views
<!-- markdownlint-disable MD033 -->
<div align="center">
<img src="https://via.placeholder.com/400x250/1e293b/06b6d4?text=Statistics+Panel" width="400" alt="Statistics Panel">
<img src="https://via.placeholder.com/400x250/1e293b/06b6d4?text=Violations+Log" width="400" alt="Violations Log">
</div>
<!-- markdownlint-enable MD033 -->

## ğŸ—ï¸ Architecture

```mermaid
graph LR
    A[Your LLM] -->|Responses| B[Monitor Service]
    B -->|Telemetry| C[FastAPI Backend]
    C -->|Stats| D[SQLite DB]
    C -->|Real-time| E[React Dashboard]
    C -->|Violations| F[Alert Service]
    F -->|Notifications| G[Email/Slack]
```

## ğŸ› ï¸ Configuration

### Basic Configuration

```yaml
# config.yaml
monitor:
  interval: 2  # seconds between checks
  provider: ollama
  model: llama3.2
  
alerts:
  email:
    enabled: true
    smtp_server: smtp.gmail.com
    recipients: ["ops@company.com"]
  
  slack:
    enabled: true
    webhook_url: https://hooks.slack.com/services/YOUR/WEBHOOK

thresholds:
  latency_warn: 2.0  # seconds
  latency_critical: 5.0
```

## ğŸ“ˆ Proven Results

> "LLMscope caught a memory leak in our RAG pipeline that would have taken down production. The Nelson Rules detected the pattern 12 minutes before our traditional monitoring would have triggered."  
> â€” *DevOps Lead, YC Startup*

> "We reduced our P99 latency by 40% after LLMscope revealed hidden patterns in our model serving."  
> â€” *ML Engineer, Fortune 500*

## ğŸš¦ Roadmap

### Current (v0.2.0)

- âœ… Real-time SPC monitoring
- âœ… Nelson Rules R1-R3
- âœ… Email/Slack alerts
- âœ… Multi-model support
- âœ… Docker deployment

### Coming Soon (v0.3.0 - Q1 2026)

- ğŸ”„ Complete Nelson Rules (R4-R8)
- ğŸ”„ Prometheus/Grafana integration
- ğŸ”„ Cost analytics dashboard
- ğŸ”„ Team collaboration features

### Future (v1.0.0 - 2026)

- ğŸ”® Manufacturing IoT monitoring
- ğŸ”® Predictive maintenance AI
- ğŸ”® Enterprise SSO/RBAC
- ğŸ”® Cloud SaaS offering

## ğŸ¤ Contributing

We love contributions! See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

### Quick Contribution Ideas

- Add support for your LLM provider
- Improve violation detection algorithms  
- Create custom dashboards
- Write documentation
- Report bugs

## ğŸ“œ License

LLMscope is licensed under the Business Source License 1.1. See [LICENSE](./LICENSE-BSL.txt) for details.

**TL;DR:** Free for production use, becomes MIT license in 2028. Commercial support available.

## ğŸ’¬ Community & Support

- **Email**: <bbaker@blb3dprinting.com>
- **GitHub Issues**: [Report bugs](https://github.com/Blb3D/LLMscope/issues)
- **GitHub Discussions**: [Feature requests & questions](https://github.com/Blb3D/LLMscope/discussions)
- **LinkedIn**: [Professional profile coming soon]
- **Website**: [Domain registration in progress]

## ğŸ™ Acknowledgments

Built with inspiration from:

- Statistical Process Control pioneers (Shewhart, Deming, Nelson)
- The Ollama community
- Modern observability tools

## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=Blb3D/LLMscope&type=Date)](https://star-history.com/#Blb3D/LLMscope&Date)

---

<div align="center">
  
**If LLMscope helps you catch issues before they impact users, please â­ star this repo!**

Made with â¤ï¸ for the AI reliability community

[ğŸš€ Get Started](https://github.com/Blb3D/LLMscope) | [ğŸ“– Read Docs](./docs) | [ï¿½ Contact](./CONTACT.md)

</div>
