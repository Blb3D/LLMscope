# LLMscope

**Real-time Statistical Process Control (SPC) monitoring for LLM latency and performance.**

> âš ï¸ **BETA VERSION 0.2.0** - Currently in testing phase. We welcome testers and feedback!  
> Free for personal/non-commercial use. [Commercial licensing available](#-license).

[![License: BSL 1.1](https://img.shields.io/badge/License-BSL%201.1-blue.svg)](LICENSE)
![Version](https://img.shields.io/badge/version-0.2.0--beta-orange)
![Status](https://img.shields.io/badge/status-beta-yellow)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-required-blue.svg)](https://www.docker.com/)

> Detect when your LLM service goes out of control using industry-standard Nelson Rules. Self-hosted, privacy-first, and production-ready.

---

## ðŸŽ¯ Why LLMscope?

LLM performance isn't just slow or fastâ€”it's **statistically predictable**. LLMscope applies **Statistical Process Control (SPC)**, the same methodology used in manufacturing quality control for decades, to monitor LLM latency in real-time.

### The Problem
- **Generic monitoring** only tells you "latency is high"
- **Cloud dashboards** require sending your data to third parties
- **No context** around what caused performance degradation
- **False alarms** from naive threshold alerts

### The Solution
- âœ… **SPC-based detection** - Nelson Rules identify real anomalies, not noise
- âœ… **Local-first** - All data stays on your infrastructure
- âœ… **Root cause analysis** - System telemetry (CPU, GPU, Memory) captured at violation time
- âœ… **Multi-provider** - Works with Ollama, OpenAI, Anthropic, and any LLM API
- âœ… **Production-ready** - Docker Compose setup in <15 minutes

---

## ðŸ”¥ Real-World Example: Cognitive Load Spike

When testing with complex prompts, we observed latency spikes that correlated directly with task complexity:

| Prompt Complexity | Baseline Latency | Spike Latency | Increase |
|-------------------|------------------|---------------|----------|
| Simple (1 sentence) | 2.0s | 2.0s | 0% |
| Medium (paragraph) | 2.0s | 4.0s | **+100%** |
| Complex (1200-page story) | 2.0s | **9.0s** | **+350%** |

**LLMscope detected this immediately** using Rule R1 (point beyond 3Ïƒ from mean) and provided full context:
- Latency jumped to 1772ms (violation)
- Rule R3 also triggered (6+ points in increasing trend)
- System telemetry showed GPU at 0% (CPU bottleneck)
- Violation modal showed Â±10 points of context for debugging

![Cognitive Load Spike Detection](docs/assets/cognitive-load-spike.png)

*Real screenshot from LLMscope detecting Claude API latency spike during complex prompt generation.*

---

## ðŸ“Š Features

### Phase 1 (Released)
- âœ… Real-time SPC chart with UCL/LCL control limits
- âœ… Nelson Rules violation detection (R1, R2, R3)
- âœ… System telemetry (CPU, GPU, Memory)
- âœ… Multi-provider support (Ollama, OpenAI, Anthropic)
- âœ… Time-window filtering (1h, 6h, 24h)
- âœ… SQLite persistence

### Phase 2 (Current - v0.2.0)
- âœ… **Server-side violation detection** - Backend calculates violations, not just frontend
- âœ… **Violation details modal** - Click any violation for full context (Â±10 points)
- âœ… **CSV export** - Download violation logs for reporting
- âœ… **Violation log** - Persistent record of all SPC rule triggers
- âœ… **Email/Slack alerts** (âš ï¸ *beta - still testing*)
- âœ… **Setup wizard** (âš ï¸ *beta - still testing*)

### Phase 3 (Planned)
- ðŸ”„ Advanced Nelson Rules (R4-R8)
- ðŸ”„ Custom alert thresholds
- ðŸ”„ Multi-model comparison
- ðŸ”„ Historical trend analysis
- ðŸ”„ Prometheus/Grafana integration

[Full Roadmap â†’](docs/ROADMAP_v5.md)

---

## ðŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Ollama running locally (or API keys for OpenAI/Anthropic)
- 2GB RAM, 1GB disk space

### Installation (< 15 minutes)

```bash
# 1. Clone the repository
git clone https://github.com/yourusername/llmscope.git
cd llmscope

# 2. Configure environment
cp .env.example .env
# Edit .env with your settings:
# - OLLAMA_BASE_URL=http://host.docker.internal:11434
# - OLLAMA_MODEL=llama3
# - LLMSCOPE_API_KEY=your-secure-key

# 3. Start all services
docker-compose up -d

# 4. Open dashboard
open http://localhost:8081
```

**That's it!** LLMscope is now monitoring your LLM and detecting violations in real-time.

---

## ðŸ“ How It Works

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      YOUR INFRASTRUCTURE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Ollama (GPU)     â”‚ â—„â”€â”€â”€â”€â”€  â”‚  Monitor Container       â”‚ â”‚
â”‚  â”‚ Port 11434       â”‚         â”‚  - Tests every 2s        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  - Measures latency      â”‚ â”‚
â”‚                               â”‚  - Collects telemetry    â”‚ â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚ POST /api/stats   â”‚
â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                               â”‚  Backend Container       â”‚ â”‚
â”‚                               â”‚  - FastAPI + SQLite      â”‚ â”‚
â”‚                               â”‚  - Calculates SPC stats  â”‚ â”‚
â”‚                               â”‚  - Nelson Rules engine   â”‚ â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚ GET /api/stats/spcâ”‚
â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                               â”‚  Frontend Container      â”‚ â”‚
â”‚  Your Browser â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  - React + Recharts      â”‚ â”‚
â”‚  localhost:8081               â”‚  - Real-time chart       â”‚ â”‚
â”‚                               â”‚  - Violation log         â”‚ â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Statistical Process Control (SPC)

LLMscope uses **control limits** calculated from your actual data:

- **Mean (Î¼)** = average latency
- **Std Dev (Ïƒ)** = spread of latency
- **UCL** = Î¼ + 3Ïƒ (Upper Control Limit)
- **LCL** = Î¼ - 3Ïƒ (Lower Control Limit)

**Why 3Ïƒ?**  
In a normal distribution, 99.7% of data falls within 3Ïƒ. Points outside this range have only a 0.3% chance of being random noiseâ€”they're **real anomalies**.

### Nelson Rules (Violation Detection)

| Rule | Condition | What It Detects |
|------|-----------|----------------|
| **R1** | Point beyond 3Ïƒ | Outlier - extreme latency spike |
| **R2** | 9+ points on same side of mean | Sustained shift - process changed |
| **R3** | 6+ points in trend (up/down) | Drift - gradual degradation |

*Advanced rules (R4-R8) coming in Phase 3*

---

## ðŸ“– Documentation

- **[Architecture Guide](docs/llmscope_architecture_guide.md)** - Deep dive into system design
- **[Roadmap](docs/ROADMAP_v5.md)** - Feature timeline and vision
- **[Scope Document](docs/SCOPE_v5.md)** - Technical specifications
- **[Case Study: Cognitive Load Spike](docs/CASE_STUDY_Cognitive_Load_Spike_RevA.md)** - Real-world example with data

---

## ðŸ› ï¸ Technology Stack

**Backend:**
- FastAPI (Python 3.11)
- SQLite (persistence)
- Uvicorn (ASGI server)
- psutil + pynvml (system metrics)

**Frontend:**
- React 18
- Recharts (visualization)
- Tailwind CSS 4
- Vite (build tool)

**Infrastructure:**
- Docker + Docker Compose
- Nginx (reverse proxy)
- Multi-stage builds (optimized images)

---

## ðŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup

```bash
# Backend (FastAPI)
cd backend
pip install -r requirements.txt
uvicorn app:app --reload --port 8000

# Frontend (React)
cd frontend
npm install
npm run dev
```

---

## ðŸ“Š Use Cases

### For AI Engineers
- Detect model degradation before users complain
- Identify which prompts cause performance issues
- Optimize infrastructure based on real telemetry

### For DevOps Teams
- Monitor LLM APIs like any other service
- Set up alerts for SPC violations
- Export violation data for postmortems

### For Researchers
- Study LLM performance characteristics
- Correlate latency with prompt complexity
- Publish reproducible performance benchmarks

---

## ðŸ”’ Privacy & Security

- âœ… **Self-hosted** - All data stays on your infrastructure
- âœ… **No telemetry** - We don't collect anything
- âœ… **API key protected** - Backend requires Bearer token
- âœ… **Prompt hashing** - Store SHA-256 hashes, not full text (optional)

---

## ðŸ“œ License

**LLMscope is source-available under the Business Source License 1.1 (BSL).**

### âœ… Free for:
- Personal use, education, testing, and evaluation
- Non-commercial projects and research
- Individual developers learning and experimenting

### âŒ Requires Commercial License for:
- Production use in commercial environments
- Offering LLMscope as a hosted/managed service
- Incorporating LLMscope into commercial products
- Using LLMscope to monitor commercial LLM deployments

### ðŸ’° Commercial Licensing

For commercial use, we offer flexible licensing options for businesses of all sizes.

**Contact:** bbaker@blb3dprinting.com

### â° Future: Automatic Open Source

On **October 29, 2028** (3 years from publication), LLMscope automatically converts to the MIT License, becoming fully open source.

**Full terms:** See [LICENSE](LICENSE) for complete details.

---

### Why BSL?

We chose BSL to:
- âœ… Keep source code visible and auditable
- âœ… Allow free use for testing and evaluation
- âœ… Protect against commercial exploitation
- âœ… Ensure long-term sustainability
- âœ… Guarantee eventual open source release

---

## ðŸŒŸ Star History

If LLMscope helps you catch performance issues before your users do, consider giving us a star! â­

---

## ðŸ“¬ Contact

- **Issues & Bugs:** [GitHub Issues](https://github.com/Blb3D/LLMscope/issues)
- **Discussions:** [GitHub Discussions](https://github.com/Blb3D/LLMscope/discussions)
- **Commercial Licensing:** bbaker@blb3dprinting.com
- **General Inquiries:** bbaker@blb3dprinting.com

---

**Built with â¤ï¸ by engineers who are tired of reactive monitoring.**
