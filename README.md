# ðŸ§± LLMscope v2.1

Local-first AI performance dashboard

## Overview

LLMscope lets you measure and visualize AI latency locally with no cloud telemetry.  
It uses a CLI sampler that writes to `Logs/chatgpt_speed_log.csv` and a FastAPI + Plotly dashboard to display live metrics.

## Run the Sampler

```bash
python chatgpt_speed_monitor_v1.py

---

## ðŸ” Featured Case Study: Cognitive Load Latency Spike  
**Captured Live â€” October 24, 2025**

> *When prompted to write a 1500-page story, LLMscope detected a 3Ïƒ latency surge â€”  
revealing the hidden cost of cognitive load in large language models.*

[![Cognitive Load Latency Spike â€“ LLMscope Dashboard](docs/images/cognitive_load_spike_thumb.png)](docs/CASE_STUDY_Cognitive_Load_Spike_RevA.md)

**Key Findings**
- Latency spiked from baseline **2 s â†’ 9 s**, then recovered dynamically  
- SPC analytics triggered a **Nelson Rule 1 violation (> 3Ïƒ)**  
- Confirmed detection of reasoning-induced latency, not network noise  

**â†’ [Read Full Case Study â†’](docs/CASE_STUDY_Cognitive_Load_Spike_RevA.md)**  

---

