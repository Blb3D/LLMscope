# LLMscope AI Copilot Configuration

# Ollama Connection
COPILOT_OLLAMA_URL=http://localhost:11434
COPILOT_MODEL=llama3.2:3b
COPILOT_FALLBACK_MODEL=llama3.2:1b

# Copilot Features
COPILOT_ENABLED=true
COPILOT_AUTO_EXPLAIN=false
COPILOT_CACHE_EXPLANATIONS=true
COPILOT_MAX_RESPONSE_LENGTH=500

# Performance Settings
COPILOT_TIMEOUT_SECONDS=30
COPILOT_TEMPERATURE=0.2
COPILOT_MAX_TOKENS=400

# UI Settings
COPILOT_AUTO_OPEN=false
COPILOT_TYPING_ANIMATION=true
COPILOT_REFRESH_INTERVAL=30

# Model Recommendations by Use Case
# Fast & Lightweight: llama3.2:1b (best for basic explanations)
# Balanced: llama3.2:3b (recommended default)
# High Quality: qwen2.5:7b (slower but more detailed)
# Specialized: codellama:7b (good for technical remediation)