# LLMscope Audit Snapshot

- Generated: 2025-10-21T11:14:38
- Root: `C:\Users\brand\OneDrive\Documents\LLMscope-main\LLMscope_Clean_Baseline_v2.1`

## Directory Overview

- Web\__pycache__\app.cpython-313.pyc  (6158 bytes)
- Web\app.py  (4617 bytes)
- Web\static\dashboard.css  (204 bytes)
- Web\static\dashboard.js  (554 bytes)
- Web\templates\dashboard.html  (340 bytes)
- Web\templates\reports.html  (2058 bytes)
- Reports\__init__.py  (0 bytes)
- Reports\__pycache__\__init__.cpython-313.pyc  (193 bytes)
- Reports\__pycache__\reports_generator.cpython-313.pyc  (5321 bytes)
- Reports\exports\LLMscope_Report_20251020_2024.pdf  (1696 bytes)
- Reports\exports\LLMscope_Report_20251020_2025.pdf  (1698 bytes)
- Reports\exports\LLMscope_Report_20251020_2027.pdf  (1697 bytes)
- Reports\exports\LLMscope_Report_20251021_0006.pdf  (1700 bytes)
- Reports\exports\LLMscope_Report_20251021_0012.pdf  (1698 bytes)
- Reports\exports\LLMscope_Report_20251021_0013.pdf  (1700 bytes)
- Reports\exports\LLMscope_Report_20251021_0016.pdf  (28391 bytes)
- Reports\exports\LLMscope_Report_20251021_0016.png  (38992 bytes)
- Reports\exports\LLMscope_Report_20251021_0017.pdf  (28391 bytes)
- Reports\exports\LLMscope_Report_20251021_0017.png  (38992 bytes)
- Reports\exports\LLMscope_Report_20251021_0018.pdf  (28391 bytes)
- Reports\exports\LLMscope_Report_20251021_0018.png  (38992 bytes)
- Reports\exports\LLMscope_Report_20251021_0029.pdf  (28393 bytes)
- Reports\exports\LLMscope_Report_20251021_0029.png  (38992 bytes)
- Reports\exports\LLMscope_Report_20251021_0030.pdf  (28391 bytes)
- Reports\exports\LLMscope_Report_20251021_0030.png  (38992 bytes)
- Reports\exports\LLMscope_Report_20251021_0031.pdf  (28391 bytes)
- Reports\exports\LLMscope_Report_20251021_0031.png  (38992 bytes)
- Reports\exports\temp_plot.png  (75647 bytes)
- Reports\reports_generator.py  (3479 bytes)
- Reports\summary_report_template.md  (1606 bytes)
- Logs\chatgpt_speed_log.csv  (6037 bytes)
- Data\metrics.db  (0 bytes)

## File Hashes (key files)

- `Web/app.py`  sha1: `69341718ff14197a840661e0296127763a071c40`  size: 4617 bytes
- `Web/templates/dashboard.html`  sha1: `74573cd198d6a096a30c8d033631f498e4c32443`  size: 340 bytes
- `Web/templates/reports.html`  sha1: `6a1bc2ecde1915623292ee558bfdaaa61e49145a`  size: 2058 bytes
- `Web/static/dashboard.js`  sha1: `6151fc136a85d89a10cb83492d69cc82e7b98033`  size: 554 bytes
- `Web/static/dashboard.css`  sha1: `678055eb44fdc830a17b52e791e3148fa0f411b1`  size: 204 bytes
- `Reports/reports_generator.py`  sha1: `2d5205633cedc0d8d18b0db3b0885c706032bdb7`  size: 3479 bytes
- `Reports/summary_report_template.md`  sha1: `3cdcbf49021e468a8837262f78731e3debb7dc57`  size: 1606 bytes
- `requirements.txt`  sha1: `576cb7be7163d85e9a51ed7ff12f1063f763b9a9`  size: 42 bytes
- `run_wrapper.py`  sha1: `d45e0997fa6b78afc74d348f3f37b88ea6b2da44`  size: 427 bytes
- `chatgpt_speed_monitor_v1.py`  sha1: `0659f09e98098eb3eeaa9800d10adaf7e7c63dad`  size: 854 bytes
- `README.md`  sha1: `0c03e5ced3d310908962094b6d01794017a21cf9`  size: 347 bytes

## Key File Contents


### Web/app.py

```
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pathlib import Path
import csv
import time

# ============================================================
# APP INITIALIZATION
# ============================================================

app = FastAPI(title="LLMscope Dashboard")

# Directories
WEB_DIR = Path(__file__).resolve().parent
TEMPLATES = Jinja2Templates(directory=str(WEB_DIR / "templates"))
STATIC = WEB_DIR / "static"
LOG_FILE = Path("Logs/chatgpt_speed_log.csv")
EXPORTS_DIR = Path("Reports/exports")

# Ensure exports folder exists
EXPORTS_DIR.mkdir(parents=True, exist_ok=True)

# Mount static assets
app.mount("/static", StaticFiles(directory=STATIC), name="static")

# ============================================================
# DASHBOARD ROUTES
# ============================================================

@app.get("/", response_class=HTMLResponse)
@app.get("/dashboard", response_class=HTMLResponse)
def dashboard(request: Request):
    """Main live dashboard"""
    return TEMPLATES.TemplateResponse("dashboard.html", {"request": request})

# ============================================================
# API ENDPOINT – LIVE DATA
# ============================================================

@app.get("/api/live", response_class=JSONResponse)
def api_live():
    """Return latest latency samples"""
    if not LOG_FILE.exists():
        return {"status": "waiting", "message": "No samples yet."}

    samples = []
    try:
        with open(LOG_FILE, "r", newline="", encoding="utf-8", errors="ignore") as f:
            reader = csv.reader(f)
            for row in reader:
                if len(row) == 2:
                    samples.append(row)
    except Exception as e:
        print(f"[WARN] Could not read CSV: {e}")
        return {"status": "error", "message": str(e)}

    if not samples:
        return {"status": "waiting", "message": "No valid data."}

    timestamps, latencies = zip(*samples[-20:])
    return {
        "status": "ok",
        "timestamps": list(timestamps),
        "latencies": [float(x) for x in latencies],
    }

# ============================================================
# REPORT ROUTES – PHASE 5C (Updated with View/Download)
# ============================================================

from fastapi.responses import FileResponse, RedirectResponse
from Reports.reports_generator import generate_report

@app.get("/reports", response_class=HTMLResponse)
def reports(request: Request):
    """Display list of generated reports"""
    EXPORTS_DIR.mkdir(parents=True, exist_ok=True)
    items = []

    for p in sorted(EXPORTS_DIR.glob("*.pdf"), key=lambda x: x.stat().st_mtime, reverse=True):
        stat = p.stat()
        items.append({
            "name": p.name,
            "created_at": time.strftime("%Y-%m-%d %H:%M", time.localtime(stat.st_mtime)),
            "size_kb": max(1, stat.st_size // 1024),
        })

    return TEMPLATES.TemplateResponse("reports.html", {
        "request": request,
        "exports": items
    })


@app.post("/reports/generate")
def reports_generate():
    """Trigger PDF generation from sampler CSV"""
    try:
        output = generate_report()
        if output:
            print(f"[INFO] Report generated: {output}")
        else:
            print("[WARN] No report generated (missing CSV data).")
    except Exception as e:
        print(f"[ERROR] Report generation failed: {e}")

    # Redirect back to reports page
    return RedirectResponse(url="/reports", status_code=303)


@app.get("/reports/download/{name}")
def reports_download(name: str):
    """Download an existing report"""
    file_path = EXPORTS_DIR / name
    if not file_path.exists():
        return {"error": "File not found"}
    return FileResponse(path=file_path, filename=name, media_type="application/pdf")


@app.get("/reports/view/{name}")
def reports_view(name: str):
    """View PDF inline in browser (no download prompt)"""
    file_path = EXPORTS_DIR / name
    if not file_path.exists():
        return {"error": "File not found"}
    headers = {"Content-Disposition": f'inline; filename="{name}"'}
    return FileResponse(path=file_path, media_type="application/pdf", headers=headers)

# ============================================================
# SERVER ENTRY POINT
# ============================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=5000)

```

### Web/templates/dashboard.html

```
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>LLMscope - Dashboard</title>
  <link rel="stylesheet" href="/static/dashboard.css">
</head>
<body>
  <h1>LLMscope – See your AI’s heartbeat</h1>
  <div id="chart"><p>Waiting for samples...</p></div>
  <script src="/static/dashboard.js"></script>
</body>
</html>

```

### Web/templates/reports.html

```
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>LLMscope Reports</title>
  <link rel="stylesheet" href="/static/dashboard.css">
  <style>
    body {
      background-color: #1A0F08;
      color: #F4C98A;
      font-family: "Inter", sans-serif;
      text-align: center;
      margin: 0;
      padding: 0;
    }
    h1 {
      color: #D37E3E;
      margin-top: 50px;
    }
    p.subtitle {
      margin-bottom: 30px;
    }
    .btn {
      display: inline-block;
      margin: 5px;
      padding: 10px 18px;
      border-radius: 8px;
      text-decoration: none;
      font-weight: 600;
      transition: all 0.2s ease;
    }
    .btn.bronze {
      background-color: #D37E3E;
      color: #1A0F08;
    }
    .btn.ghost {
      border: 1px solid #D37E3E;
      color: #D37E3E;
    }
    .btn.small {
      font-size: 0.9rem;
      padding: 6px 12px;
    }
    .actions-inline {
      display: inline-flex;
      gap: 8px;
      margin-left: 10px;
    }
    ul {
      list-style: none;
      padding: 0;
      margin-top: 40px;
    }
    li {
      margin: 10px 0;
      font-size: 0.95rem;
    }
  </style>
</head>
<body>
  <h1>Reports</h1>
  <p class="subtitle">Generate and download PDF summaries of recent runs.</p>

  <form method="post" action="/reports/generate">
    <button type="submit" class="btn bronze">Generate New Report</button>
  </form>

  {% if exports %}
    <h2 style="color:#D37E3E;margin-top:40px;">Existing Reports</h2>
    <ul>
      {% for item in exports %}
        <li>
          {{ item.name }} ({{ item.size_kb }} KB, created {{ item.created_at }})
          <div class="actions-inline">
            <a href="/reports/view/{{ item.name }}" target="_blank" class="btn small ghost">View</a>
            <a href="/reports/download/{{ item.name }}" class="btn small bronze">Download</a>
          </div>
        </li>
      {% endfor %}
    </ul>
  {% else %}
    <p style="margin-top: 40px;">No reports yet — click <em>Generate New Report</em> to create your first one.</p>
  {% endif %}
</body>
</html>

```

### Web/static/dashboard.js

```
async function fetchLiveData() {
  try {
    const res = await fetch("/api/live");
    const data = await res.json();
    const chart = document.getElementById("chart");
    if (data.status === "ok") {
      const latest = data.latencies[data.latencies.length - 1];
      chart.innerHTML = `<h3>Latest Latency: ${latest}s</h3>`;
    } else {
      chart.innerHTML = `<p>${data.message || "Waiting for samples..."}</p>`;
    }
  } catch (err) {
    console.error("Error fetching live data", err);
  }
}

setInterval(fetchLiveData, 2000);
fetchLiveData();

```

### Web/static/dashboard.css

```
body {
  background-color: #1A0F08;
  color: #F4C98A;
  font-family: Inter, sans-serif;
  text-align: center;
  padding: 40px;
}

h1 {
  color: #D37E3E;
}

h3 {
  font-size: 1.5rem;
  margin-top: 2rem;
}

```

### Reports/reports_generator.py

```
from pathlib import Path
import pandas as pd
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib.utils import ImageReader
from datetime import datetime
import plotly.graph_objects as go

LOG_FILE = Path("Logs/chatgpt_speed_log.csv")
EXPORTS_DIR = Path("Reports/exports")
EXPORTS_DIR.mkdir(parents=True, exist_ok=True)

def generate_report():
    """Generate a summary PDF report from chatgpt_speed_log.csv with Plotly chart."""
    print("[REPORT] Starting report generation...")

    if not LOG_FILE.exists() or LOG_FILE.stat().st_size == 0:
        print("[ERROR] Log file missing or empty.")
        return None

    try:
        df = pd.read_csv(LOG_FILE, names=["timestamp", "latency"], on_bad_lines="skip")
        df = df.dropna()
        df["latency"] = pd.to_numeric(df["latency"], errors="coerce")
        df = df.dropna(subset=["latency"])
        print(f"[REPORT] Loaded {len(df)} rows.")
    except Exception as e:
        print(f"[ERROR] CSV read failed: {e}")
        return None

    if df.empty:
        print("[ERROR] No valid data.")
        return None

    avg_latency = round(df["latency"].mean(), 2)
    latest_latency = round(df["latency"].iloc[-1], 2)
    total_samples = len(df)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    pdf_path = EXPORTS_DIR / f"LLMscope_Report_{timestamp}.pdf"
    chart_path = EXPORTS_DIR / f"LLMscope_Report_{timestamp}.png"

    # --- Generate Plotly line chart ---
    try:
        fig = go.Figure()
        fig.add_trace(go.Scatter(
            x=df["timestamp"],
            y=df["latency"],
            mode="lines+markers",
            line=dict(color="#D37E3E", width=2),
            marker=dict(size=4),
            name="Latency (s)"
        ))
        fig.update_layout(
            title="LLMscope Latency Trend",
            xaxis_title="Timestamp",
            yaxis_title="Latency (s)",
            template="plotly_dark",
            paper_bgcolor="#1A0F08",
            plot_bgcolor="#1A0F08",
            font=dict(color="#F4C98A")
        )
        fig.write_image(str(chart_path))
        print(f"[REPORT] Chart exported → {chart_path}")
    except Exception as e:
        print(f"[ERROR] Failed to generate Plotly chart: {e}")
        chart_path = None

    # --- Create PDF ---
    try:
        c = canvas.Canvas(str(pdf_path), pagesize=letter)
        width, height = letter

        c.setTitle("LLMscope Report")
        c.setFont("Helvetica-Bold", 20)
        c.setFillColorRGB(0.83, 0.49, 0.24)
        c.drawString(72, height - 72, "LLMscope Performance Report")

        c.setFont("Helvetica", 12)
        c.setFillColorRGB(1, 1, 1)
        c.drawString(72, height - 110, f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
        c.drawString(72, height - 130, f"Total Samples: {total_samples}")
        c.drawString(72, height - 150, f"Average Latency: {avg_latency}s")
        c.drawString(72, height - 170, f"Latest Latency: {latest_latency}s")

        if chart_path and chart_path.exists():
            img = ImageReader(str(chart_path))
            c.drawImage(img, 72, 150, width=460, preserveAspectRatio=True)

        c.showPage()
        c.save()
        print(f"[REPORT] PDF generated → {pdf_path}")
    except Exception as e:
        print(f"[ERROR] PDF creation failed: {e}")
        return None

    return pdf_path

```

### Reports/summary_report_template.md

```
# LLMscope — Run Summary Report (Template)

> **Version:** v2.1 • **Theme:** BLB3D Bronze/Black (#D37E3E / #1A0F08)  
> Replace the curly-brace placeholders during report generation.

---

## 1) Overview
- **Report ID:** {{ report_id }}
- **Generated At:** {{ generated_at }}
- **Run Window:** {{ run_window_start }} → {{ run_window_end }}
- **Sample Count:** {{ sample_count }}

---

## 2) Latency Snapshot
| Metric | Value |
|---|---|
| Min (ms) | {{ latency_min_ms }} |
| P50 (ms) | {{ latency_p50_ms }} |
| P90 (ms) | {{ latency_p90_ms }} |
| P95 (ms) | {{ latency_p95_ms }} |
| Max (ms) | {{ latency_max_ms }} |
| Mean (ms) | {{ latency_mean_ms }} |
| Std Dev (ms) | {{ latency_std_ms }} |

> Source: `Logs/chatgpt_speed_log.csv`

---

## 3) Throughput & Health
- **Samples / min:** {{ samples_per_min }}
- **Gaps detected:** {{ gaps_count }}
- **Nelson-rule alerts (if implemented):** {{ nelson_alerts_count }}

---

## 4) Trend & Distribution
The generation script should embed a **trend chart** (latency over time) and a **distribution chart** (histogram/PDF).  
When exporting HTML/PDF, inject images or inline Plotly HTML here.

**Trend (inline HTML or static image):**
{{ chart_trend_html_or_img }}

**Distribution (inline HTML or static image):**
{{ chart_distribution_html_or_img }}

---

## 5) System Footprint (Optional)
- **RAM (MB):** {{ ram_mb }}
- **CPU (%):** {{ cpu_pct }}
- **Sampler PID:** {{ sampler_pid }}

> Only populate if the diag module is enabled.

---

## 6) Notes
{{ notes_md }}

---

### Appendix A — Generation Parameters
```json
{{ generation_params_json }}
```

```

### requirements.txt

```
fastapi
uvicorn
plotly
jinja2
pandas

```

### run_wrapper.py

```
import os

if os.environ.get("LLMSCOPE_DEV") == "1":
    print("[DEV MODE] Running from Python sources...")
    from Web import app as app_module
else:
    try:
        from Web import app as app_module
    except ImportError as e:
        raise SystemExit("Error: app not found. Ensure build is compiled.\n" + str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app_module.app, host="127.0.0.1", port=5000)

```

### chatgpt_speed_monitor_v1.py

```
import time, csv, random
from datetime import datetime
from pathlib import Path

DATA_DIR = Path("Logs")
DATA_DIR.mkdir(exist_ok=True)
CSV_FILE = DATA_DIR / "chatgpt_speed_log.csv"

def sample_latency():
    return round(random.uniform(2.0, 8.0), 3)

def safe_write(row):
    for attempt in range(3):
        try:
            with open(CSV_FILE, "a", newline="") as f:
                csv.writer(f).writerow(row)
            return
        except PermissionError:
            print("[WARN] File locked — retrying...")
            time.sleep(1)
    print("[ERROR] Unable to write; file may be open elsewhere.")

def main():
    while True:
        latency = sample_latency()
        safe_write([datetime.now().isoformat(), latency])
        print(f"[LLMscope] Logged latency: {latency} sec")
        time.sleep(5)

if __name__ == "__main__":
    main()

```

### README.md

```
# 🧱 LLMscope v2.1

Local-first AI performance dashboard

## Overview

LLMscope lets you measure and visualize AI latency locally with no cloud telemetry.  
It uses a CLI sampler that writes to `Logs/chatgpt_speed_log.csv` and a FastAPI + Plotly dashboard to display live metrics.

## Run the Sampler

```bash
python chatgpt_speed_monitor_v1.py

```