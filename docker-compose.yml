services:
  backend:
    container_name: llmscope_api
    build:
      context: .
      dockerfile: Dockerfile.backend
    env_file:
      - .env
    ports:
      - "8081:8000"
    volumes:
      - ./data:/data
      - /proc:/host_proc:ro
      - /sys:/host_sys:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  monitor:
    container_name: llmscope_monitor
    build:
      context: .
      dockerfile: Dockerfile.monitor
    env_file:
      - .env
    environment:
      - USE_OLLAMA=true
      - OLLAMA_MODEL=gemma3:4b
      - LLMSCOPE_API_KEY=dev-123
      - LLMSCOPE_API_BASE=http://backend:8000
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped

  frontend:
    container_name: llmscope_web
    build:
      context: .
      dockerfile: Dockerfile.frontend
    env_file:
      - .env
    ports:
      - "3000:80"
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped

volumes:
  data:
