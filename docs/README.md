# üß± LLMscope v2.1

Local-first AI performance dashboard

## Overview

LLMscope lets you measure and visualize AI latency locally with no cloud telemetry.  
It uses a CLI sampler that writes to `Logs/chatgpt_speed_log.csv` and a FastAPI + Plotly dashboard to display live metrics.

## Run the Sampler

```bash
python chatgpt_speed_monitor_v1.py

---

## üîç Featured Case Study: Cognitive Load Latency Spike  
**Captured Live ‚Äî October 24, 2025**

> *When prompted to write a 1500-page story, LLMscope detected a 3œÉ latency surge ‚Äî  
revealing the hidden cost of cognitive load in large language models.*

[![Cognitive Load Latency Spike ‚Äì LLMscope Dashboard](docs/images/cognitive_load_spike_thumb.png)](docs/CASE_STUDY_Cognitive_Load_Spike_RevA.md)

**Key Findings**
- Latency spiked from baseline **2 s ‚Üí 9 s**, then recovered dynamically  
- SPC analytics triggered a **Nelson Rule 1 violation (> 3œÉ)**  
- Confirmed detection of reasoning-induced latency, not network noise  

**‚Üí [Read Full Case Study ‚Üí](docs/CASE_STUDY_Cognitive_Load_Spike_RevA.md)**  

---
üß† GPU Telemetry Support (Optional Feature)
Overview

LLMscope automatically reports real-time CPU, memory, and (when available) GPU temperatures for your local environment.
By default, only CPU and RAM metrics are available inside most Docker / WSL2 setups. GPU telemetry requires additional configuration.

‚úÖ Included by Default

CPU Utilization (%) ‚Äî via psutil.cpu_percent()

Memory Utilization (%) ‚Äî via psutil.virtual_memory()

CPU Temperature (if supported) ‚Äî via psutil.sensors_temperatures()

These values are available on all platforms, including Docker Desktop with WSL2.

‚öôÔ∏è Optional: NVIDIA GPU Temperature

If you have an NVIDIA GPU (e.g., RTX 30-series) and want temperature reporting:

1Ô∏è‚É£ Install NVIDIA Container Toolkit (inside WSL2 or Linux)
sudo apt update
sudo apt install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

2Ô∏è‚É£ Enable GPU passthrough in Docker Compose

In your docker-compose.yml, under the backend service, add:

deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]

3Ô∏è‚É£ Install NVML Python bindings

Add this line to your Dockerfile.backend (after RUN pip install psutil):

RUN pip install nvidia-ml-py3

4Ô∏è‚É£ Rebuild and restart
docker-compose up -d --build backend


When configured correctly, the /api/system endpoint will return:

{
  "gpuTemp": 47.0,
  "cpuTemp": 43.6,
  "cpu": 11.3,
  "memory": 58.7
}

‚ö†Ô∏è Note for Windows / WSL2 Users

If you‚Äôre running LLMscope inside Docker Desktop on Windows, GPU telemetry may appear as null due to limited hardware access through WSL2.
CPU and memory data remain accurate.

üì¶ Optional Local Mode (without Docker)

You can also run app.py directly on your Windows or Linux host with Python ‚â•3.10 installed.
In this mode, LLMscope has direct hardware access and will display GPU temps automatically if NVML is present.

üí° Tooltip / UI Message (for Dashboard)

GPU telemetry unavailable.
This system is running in a virtualized environment without GPU passthrough.
CPU and memory statistics remain active.
See the README section ‚ÄúGPU Telemetry Support‚Äù for enablement steps.

---
üß© System Requirements & Hardware Compatibility
Component	Required	Notes	Status
CPU	‚úÖ Yes	Used for latency measurement and system telemetry	Fully supported
Memory (RAM)	‚úÖ Yes	Reported via psutil	Fully supported
GPU (NVIDIA / CUDA)	‚öôÔ∏è Optional	Enables GPU temperature monitoring via NVML (nvidia-ml-py3)	Optional
Docker Desktop / Compose	‚úÖ Yes	Required for containerized deployment	Fully supported
Python 3.10+	‚úÖ Yes	Required for backend / FastAPI	Fully supported
WSL2 (Windows only)	‚ö†Ô∏è Optional	Works for CPU/RAM telemetry, limited GPU access	Partially supported
NVIDIA Container Toolkit	‚öôÔ∏è Optional	Required for GPU passthrough inside Docker	Optional
Internet Access	‚öôÔ∏è Optional	Required for online model benchmarking (OpenAI, Anthropic, etc.)	Optional
üß† Notes

CPU/RAM telemetry works in all environments, including Docker on WSL2.

GPU telemetry requires an NVIDIA GPU and additional configuration.

GPU passthrough is disabled by default in Docker Desktop; see the GPU Telemetry Support
 section for setup instructions.

Running LLMscope natively (without Docker) provides full hardware sensor access automatically.
---